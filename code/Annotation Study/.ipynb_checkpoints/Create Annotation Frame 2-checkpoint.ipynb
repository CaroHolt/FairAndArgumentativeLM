{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from itertools import *\n",
    "from click import style\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import json_lines\n",
    "import seaborn as sns\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-barbados",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disciplinary-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/debateorg/debateorg.json\", \"r\") as d:\n",
    "    debates_dict = json.load(d)\n",
    "with open(\"../data/debateorg/users.json\", \"r\") as u:\n",
    "      users_dict = json.load(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(debates_data: dict, users_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract and combines debates and user data into a single dataframe. Return the dataframe.\n",
    "    Currently, only the birthday, education, gender and political orientation are extracted and\n",
    "    returned as user-defining features.\n",
    "    Arguments:\n",
    "    debates_data -- Dictionary containing the debates data.\n",
    "    users_data -- Dictionary containing the users and their properties.\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "    properties_of_interest = [\"birthday\", \"ethnicity\", \"gender\", \"political_ideology\", \"education\", \n",
    "                              \"interested\", \"income\", \"looking\", \"party\", \"relationship\", \"win_ratio\", \n",
    "                              \"religious_ideology\", \"number_of_all_debates\", \"big_issues_dict\"]\n",
    "\n",
    "    for key, debate in tqdm(debates_data.items()):\n",
    "        # Sometimes, the users of the debate didn't exist anymore at the time\n",
    "        # the data was collected.\n",
    "        try:\n",
    "            category = debate[\"category\"]\n",
    "        except KeyError:\n",
    "            category = None\n",
    "            \n",
    "        try:\n",
    "            title = debate[\"title\"]\n",
    "        except KeyError:\n",
    "            title = None\n",
    "        \n",
    "        try:\n",
    "            date = debate[\"start_date\"]\n",
    "        except KeyError:\n",
    "            date = None\n",
    "        \n",
    "        try:\n",
    "            user1 = users_data[debate[\"participant_1_name\"]]\n",
    "        except KeyError:\n",
    "            user1 = None\n",
    "\n",
    "        try:\n",
    "            user2 = users_data[debate[\"participant_2_name\"]]\n",
    "        except KeyError:\n",
    "            user2 = None\n",
    "\n",
    "        # If both users do not exist, skip this debate\n",
    "        if not user1 and not user2:\n",
    "            logging.debug(\"Both users are absent from debate data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # For each round in this debate...\n",
    "        for debate_round in debate[\"rounds\"]:\n",
    "            # For each argument in this round...\n",
    "            for argument in debate_round:\n",
    "                arguing_user = (\n",
    "                    user1 if argument[\"side\"] == debate[\"participant_1_position\"] else user2)\n",
    "                \n",
    "                arguing_user_name = (\n",
    "                    debate[\"participant_1_name\"] if argument[\"side\"] == debate[\"participant_1_position\"] else debate[\"participant_2_name\"])\n",
    "\n",
    "                # Skip this argument if arguing user does not exist in the dta\n",
    "                if not arguing_user:\n",
    "                    continue\n",
    "                    \n",
    "                # Filtering for votes\n",
    "                votes = []\n",
    "                for vote in debate['votes']:\n",
    "                    votes.append(vote['votes_map'][arguing_user_name])\n",
    "\n",
    "                # Filtering for relevant properties\n",
    "                properties = {\n",
    "                    key: value\n",
    "                    for key, value in arguing_user.items() if key in properties_of_interest}\n",
    "\n",
    "                # Save the text and find the political ideology of the user.\n",
    "                extracted_data.append({\n",
    "                    \"argument\": argument[\"text\"],\n",
    "                    \"title\": title,\n",
    "                    \"category\": category,\n",
    "                    \"date\": date,\n",
    "                    **properties,\n",
    "                    \"votes\": votes})\n",
    "\n",
    "    return pd.DataFrame(columns=[\"argument\", \"title\", \"category\", \"date\", *properties_of_interest, \"votes\"], data=extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debates = extract_data(debates_dict, users_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-three",
   "metadata": {},
   "source": [
    "# Load Target Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-education",
   "metadata": {},
   "source": [
    "## Queerness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "refined-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queerness_tt():\n",
    "    targets_1 = [\"gay\", \"gays\", \"lesbian\", \"lesbians\", \"bisexual\", \"bisexuals\", \"homosexual\", \"homosexuals\",\n",
    "                \"transgender\", \"transgenders\", \"sapphic\", \"sapphics\", \"pansexual\", \"pansexuals\", \"queer\", \"queers\",\n",
    "                \"genderqueer\", \"genderqueers\", \"transsexual\", \"transsexuals\", \"crosssexual\", \"crosssexuals\", \"epicene\",\n",
    "                 \"epicenes\", \"lgbtq\", \"lgbtqi\", \"hermaphroditic\", \"hermaphrodite\", \"hermaphrodites\", \"androgynous\", \"androgyne\",\n",
    "                 \"androgynes\", \"intersex\"]\n",
    "    targets_2 = [\"straight\", \"straights\", \"monosexual\", \"monosexuals\", \"heterosexual\", \"heterosexuals\", \"cisgender\",\n",
    "                \"cisgenders\", \"sexually normal\", \"hetero\", \"heteros\", \"unisexual\", \"unisexuals\"]  \n",
    "    attributes_1 = [\"mentally ill\", \"polygamous\", \"flamboyant\", \"pedophile\", \"sexual predator\", \"mental disorder\",\n",
    "                    \"irreligious\", \"villain\", \"serial killer\", \"depress\", \"adultery\", \"drug addict\", \"sadist\", \"arts\",\n",
    "                    \"artistic\", \"theatre\", \"dramatic\", \"show queen\", \"fashion\", \"predator\", \"party drugs\",\n",
    "                    \"illicit drugs\", \"alcohol\", \"sexual abuse\", \"child rapist\", \"indecisi\", \"promiscuous\",\n",
    "                    \"sexually manipulative\", \"prostitute\", \"drag queen\", \"drag king\", \"criminal\", \"hypersexual\",\n",
    "                    \"asexual\", \"pervert\", \"perverts\", \"poofter\", \"faggot\", \"fag\", \"weak\", \"timid\", \"loser\", \"loner\",\n",
    "                    \"lonely\", \"sin\", \"disease\", \"confuse\", \"strange\", \"weird\", \"coward\", \"sexually abnormal\",\n",
    "                    \"immoral\", \"insecure\", \"repulsive\", \"frustrat\", \"sinful\", \"sensitive\", \"weak minded\", \"lack self control\",\n",
    "                    \"oversex\", \"submissive\", \"emotional\"]\n",
    "    attributes_2 = [\"mentally\", \"strong\", \"monogamous\", \"modest\", \"teleiophile\", \"protector\", \"mental stability\",\n",
    "                    \"hero\", \"police officer\", \"cheerful\", \"healthy\", \"compassionate\", \"scientific\", \"science\", \"subtle\",\n",
    "                    \"unfashion\", \"defender\", \"determined\", \"moral\", \"chaste\", \"cisgender\", \"lawful\", \"decent\", \"saint\",\n",
    "                    \"faggot\", \"strong\", \"daring\", \"winner\", \"cheerful\", \"virtue\", \"health\", \"assured\", \"normal\",\n",
    "                    \"normal\", \"brave\", \"moral\", \"confident\", \"delightful\", \"satisfied\", \"innocent\", \"tough\",\n",
    "                    \"dominating\", \"unemotional\"]\n",
    "    return targets_1, targets_2, attributes_1, attributes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-creation",
   "metadata": {},
   "source": [
    "## Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "seventh-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "def religious_tt():\n",
    "    targets_1 = [\"muslim\", \"muslims\", \"islam\", \"islamic\", \"islamism\", \"arab\", \"arabs\", \"quran\", \"koran\"]\n",
    "    targets_2 = [\"christian\", \"christians\", \"christianity\", \"church\", \"bible\", \"priest\"]   \n",
    "    attributes_1 = [\"terrorist\", \"terrorism\", \"threat\", \"threaten\", \"dangerous\", \"deadly\", \"attack\", \"bomb\", \"murder\",\n",
    "                    \"hijack\", \"criminal\", \"enemy\", \"war\", \"enemies\", \"extremist\", \"radical\", \"hate\", \"rich\", \"lazy\",\n",
    "                    \"flashy\", \"lazy sheik\", \"oil sheik\", \"rapist\", \"slaves\", \"shooter\", \"belly dancers\", \"hijab\",\n",
    "                    \"viel\", \"vielded\", \"oppressed\", \"patriarchy\", \"misogyny\", \"patriarchal\", \"deport\", \"detain\",\n",
    "                    \"thieves\", \"thug\", \"charlatan\", \"brute\", \"power hungry\", \"beard\", \"wealthy oilmen\",\n",
    "                    \"harem maiden\", \"suicide bomb\", \"headscarves\", \"violence\", \"violent\", \"hostile\", \"sinister\"]\n",
    "    attributes_2 = [\"police\", \"nonviolence\", \"safety\", \"defend\", \"helpful\", \"safe\", \"defend\", \"build\", \"restore\",\n",
    "                    \"relinquish\", \"lawful\", \"friend\", \"peace\", \"friends\", \"moderate\", \"conservative\", \"love\", \"poor\",\n",
    "                    \"energetic\", \"simple\", \"defender\", \"freemen\", \"hero\", \"hat\", \"unviel\", \"unvielded\", \"rejoiced\",\n",
    "                    \"matriarchy\", \"philogyny\", \"matriarchal\", \"admit\", \"liberate\", \"police\", \"hero\", \"honest\", \"mild\",\n",
    "                    \"gratified\", \"clean shave\", \"negotiator\", \"compassion\", \"gentle\", \"kind\", \"happy\"]\n",
    "    return targets_1, targets_2, attributes_1, attributes_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-apache",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "local-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of words in string\n",
    "def countWords(string):\n",
    "    state = 'OUT'\n",
    "    wc = 0\n",
    " \n",
    "    # Scan all characters one by one\n",
    "    for i in range(len(string)):\n",
    " \n",
    "        # If next character is a separator,\n",
    "        # set the state as OUT\n",
    "        if (string[i] == ' ' or string[i] == '\\n' or\n",
    "            string[i] == '\\t'):\n",
    "            state = 'OUT'\n",
    " \n",
    "        # If next character is not a word\n",
    "        # separator and state is OUT, then\n",
    "        # set the state as IN and increment\n",
    "        # word count\n",
    "        elif state == 'OUT':\n",
    "            state = 'IN'\n",
    "            wc += 1\n",
    " \n",
    "    # Return the number of words\n",
    "    return wc\n",
    " \n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def preprocessStringToTokens(opinion_string):\n",
    "    # 1. First Lower Case everything\n",
    "    opinion_string = opinion_string.lower()\n",
    "\n",
    "    # 2. Decontract words\n",
    "    opinion_string = decontracted(opinion_string)\n",
    "\n",
    "    # 3. Remove numbers\n",
    "    opinion_string = re.sub(r'\\d+', '', opinion_string)\n",
    "\n",
    "    # 4. Remove URLs\n",
    "    opinion_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', opinion_string)\n",
    "\n",
    "    # 5. Split into sentences\n",
    "    opinion_sentences = tokenize.sent_tokenize(opinion_string)\n",
    "\n",
    "    # 6. Split into tokens and remove punctuation\n",
    "    wordMap = []\n",
    "    for s in opinion_sentences:\n",
    "        s = re.sub(r\"[^A-Za-z]+\", \" \", s)\n",
    "        wordMap.append(word_tokenize(s))\n",
    "\n",
    "    return wordMap\n",
    "\n",
    "def preprocessForAnnotation(opinion_string):\n",
    "    # 1. Lowercase letters\n",
    "    opinion_string = opinion_string.lower()\n",
    "    \n",
    "    # 2. Remove numbers\n",
    "    opinion_string = re.sub(r'\\d+', '', opinion_string)\n",
    "\n",
    "    # 3. Remove URLs\n",
    "    opinion_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', opinion_string)  \n",
    "    \n",
    "    # 4. Remove special characters\n",
    "    opinion_string = opinion_string.replace('\\n',' ')\n",
    "    opinion_string = opinion_string.replace('\\r',' ')\n",
    "    \n",
    "    # 5. Remove large white spaces    \n",
    "    opinion_string = ' '.join(opinion_string.split())\n",
    "    return opinion_string\n",
    "\n",
    "def splitIntoSentences(opinion_sentence):\n",
    "    # Split into sentences\n",
    "    opinion_sentences = tokenize.sent_tokenize(opinion_sentence)\n",
    "    \n",
    "    return opinion_sentences\n",
    "\n",
    "def preprocessStringToSentences(opinion_string):\n",
    "    # 1. First Lower Case everything\n",
    "    opinion_string = opinion_string.lower()\n",
    "\n",
    "    # 2. Decontract words\n",
    "    opinion_string = decontracted(opinion_string)\n",
    "\n",
    "    # 3. Remove numbers\n",
    "    opinion_string = re.sub(r'\\d+', '', opinion_string)\n",
    "\n",
    "    # 4. Remove URLs\n",
    "    opinion_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', opinion_string)\n",
    "\n",
    "    # 5. Split into sentences\n",
    "    opinion_sentences = tokenize.sent_tokenize(opinion_string)\n",
    "\n",
    "    # 6. Remove punctuation\n",
    "    wordMap = []\n",
    "    for s in opinion_sentences:\n",
    "        s = re.sub(r\"[^A-Za-z]+\", \" \", s)\n",
    "        wordMap.append(s)\n",
    "\n",
    "    return wordMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debates['preprocessedArgTokens'] = df_debates.progress_apply(lambda row: preprocessStringToTokens(row['argument']),axis=1)\n",
    "df_debates['annotationComment'] = df_debates.progress_apply(lambda row: preprocessForAnnotation(row['argument']),axis=1)\n",
    "df_debates['annotationSentence'] = df_debates.progress_apply(lambda row: splitIntoSentences(row['annotationComment']),axis=1)\n",
    "df_debates['preprocessedArgSentence'] = df_debates.progress_apply(lambda row: preprocessStringToSentences(row['argument']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debates = df_debates.reset_index()\n",
    "df_debates = df_debates.rename(columns = {'index':'ID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-malpractice",
   "metadata": {},
   "source": [
    "## Extract biased co-occurrence in sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bored-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, window_size):\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        center_word = words[i]\n",
    "        # Calculate context words\n",
    "        if ((i-window_size) < 0):\n",
    "            first_word_index = 0\n",
    "        else:\n",
    "            first_word_index = (i-window_size)\n",
    "        if (i + window_size) < (len(words)):\n",
    "            last_word_index = i + window_size\n",
    "        else:\n",
    "            last_word_index = len(words)\n",
    "        context_words = words[first_word_index:i] + words[(i+1):last_word_index+1]\n",
    "        yield center_word, context_words\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opposite-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cartesian product\n",
    "def cartesianProductList(list1, list2):\n",
    "    cartesianProductList = []\n",
    "    for element in itertools.product(list1, list2):\n",
    "        cartesianProductList.append(element)\n",
    "    return cartesianProductList\n",
    "\n",
    "# Count occurrences\n",
    "def countOccurrence(comment, targetWordList):\n",
    "    totalCount = 0\n",
    "    for sentence in comment: \n",
    "        for e in targetWordList:\n",
    "            temp = sentence.count(e)\n",
    "            totalCount = totalCount + temp\n",
    "    return totalCount\n",
    "\n",
    "# Check whether tuple in sentence\n",
    "def findBias(comment, cartesianProductList):\n",
    "    biasedSentences = []\n",
    "    foundTuples = []\n",
    "    for sentence in comment:\n",
    "        for tup in cartesianProductList:\n",
    "            if tup[0] in sentence and tup[1] in sentence:\n",
    "                biasedSentences.append(sentence)\n",
    "                foundTuples.append(tup)\n",
    "    return pd.Series([biasedSentences, foundTuples, len(foundTuples)])\n",
    "\n",
    "# Returns number of words in string\n",
    "def countWords(string):\n",
    "    state = 'OUT'\n",
    "    wc = 0\n",
    " \n",
    "    # Scan all characters one by one\n",
    "    for i in range(len(string)):\n",
    " \n",
    "        # If next character is a separator,\n",
    "        # set the state as OUT\n",
    "        if (string[i] == ' ' or string[i] == '\\n' or\n",
    "            string[i] == '\\t'):\n",
    "            state = 'OUT'\n",
    " \n",
    "        # If next character is not a word\n",
    "        # separator and state is OUT, then\n",
    "        # set the state as IN and increment\n",
    "        # word count\n",
    "        elif state == 'OUT':\n",
    "            state = 'IN'\n",
    "            wc += 1\n",
    " \n",
    "    # Return the number of words\n",
    "    return wc\n",
    " \n",
    "\n",
    "# Check whether context word is target word and whether attribute is in context\n",
    "def findBiasInSlidingWindow(comment, sentences, t_list, a_list, windowSize):\n",
    "    biasedSentences = []\n",
    "    foundTuples = []\n",
    "    print(comment)\n",
    "    for index, sentence in enumerate(comment):\n",
    "        for center_word, context_words in get_windows(sentence, windowSize):\n",
    "            if center_word in t_list:\n",
    "                for e in a_list:\n",
    "                    if countWords(e) > 1:\n",
    "                        split = e.split()\n",
    "                        if split[0] in context_words and split[1] in context_words:\n",
    "                            biasedSentences.append(sentences[index])\n",
    "                            foundTuples.append(tuple([e, center_word]))\n",
    "                    elif e in context_words:\n",
    "                        biasedSentences.append(sentences[index])\n",
    "                        foundTuples.append(tuple([e, center_word]))\n",
    "    return pd.Series([biasedSentences, foundTuples, len(foundTuples)])\n",
    "\n",
    "# Check whether tuple in sentence\n",
    "def findBiasCount(comment, cartesianProductList):\n",
    "    foundTuples = []\n",
    "    for sentence in comment:\n",
    "        for tup in cartesianProductList:\n",
    "            if tup[0] in sentence and tup[1] in sentence:\n",
    "                foundTuples.append(tup)\n",
    "    return len(foundTuples)\n",
    "\n",
    "def calculateSummarySlidingWindow(dataSet, targetTermFunction, windowSize):\n",
    "    t1, t2, a1, a2 = targetTermFunction()\n",
    "    t1 = [x.lower() for x in t1]\n",
    "    t2 = [x.lower() for x in t2]\n",
    "    a1 = [x.lower() for x in a1]\n",
    "    a2 = [x.lower() for x in a2]\n",
    "    df = dataSet.copy()\n",
    "\n",
    "    # Count word occurrences\n",
    "    df['minorityCount'] =  df.progress_apply(lambda row: countOccurrence(row['preprocessedArgTokens'], t1),axis=1)\n",
    "    df['majorityCount'] =  df.progress_apply(lambda row: countOccurrence(row['preprocessedArgTokens'], t2),axis=1)\n",
    "    \n",
    "\n",
    "    totalMinorityOccurrence = df['minorityCount'].sum() \n",
    "    totalMajorityOccurrence = df['majorityCount'].sum() \n",
    "\n",
    "    df[['T1 x A1 Sentences', 'T1 x A1 Tuples Found', 'T1 x A1 Count']] = df.progress_apply(lambda row: findBiasInSlidingWindow(row['preprocessedArgTokens'], row['annotationSentence'], t1, a1, windowSize),axis=1)\n",
    "   \n",
    "    t1_a1_total = df['T1 x A1 Count'].sum() \n",
    "    if (t1_a1_total > 0):\n",
    "        t1_a1_percentage = (t1_a1_total / totalMinorityOccurrence) * 100\n",
    "        t1_a1_percentage = round(t1_a1_percentage, 2)\n",
    "    else:\n",
    "        t1_a1_percentage = 0\n",
    "\n",
    "    return pd.Series([totalMinorityOccurrence, totalMajorityOccurrence, t1_a1_total, t1_a1_percentage]), df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
